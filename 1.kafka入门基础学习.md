# kafka学习

[TOC]

## 1.启动kafka服务

阿里ECS云服务地址公网地址：101.200.194.183

启动zookeeper：

```
zkServer.sh start
```

关闭zookeeper：

```
zkServer.sh stop
```

启动kafka服务：其中server.properties中配置了kafka的一些信息。

```
kafka-server-start.sh  /opt/myconfig/server.properties
```

### 1.1kafka服务端常用参数配置

#### 1.1.1 zookeeper.connect

​		在使用kafka作为发布订阅的消息系统，需要zookeeper维护各个kafka的节点。所以需要用到zookeeper服务，在kafka配置文件中需要制定zookeeper主机地址，如果zookeeper是集群则以逗号隔开，如：

```
101.200.194.183:2181,101.200.194.183:2182
```

#### 1.1.2 listeners

​		监听列表，broker对外提供服务时绑定的IP地址和端口，多个以逗号隔开，如果监听器名称不是一个安全的协议，listener.security.protocol.map也必须设置，主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。

#### 1.1.3 broker.id

​		broker的唯一标识符，表示每个节点，如果不配置则自动生成，建议配置且一定要保证集群中必须唯一，默认-1.

#### 1.1.4 log.dri

​        日志数据存放的目录，如果没有配置则使用log.dir,建议配置此项。

#### 1.1.5 message.max.bytes

​        配置服务器接受单个消息的最大大小，默认1000012约等于976.6KB。

## 2.使用kafka测试消息生产与消费

### 1.首先创建一个主题

命令如下:

```
kafka-topics.sh  --zookeeper localhost:2181 --create --topic lixiaochi --partitions 2 --replication-factor 1.

--zookeeper:指定了kafka所连接的zookeeper服务地址。
--topic：指定了所要创建主题的名称
--partitions 指定了分区的个数
--replication-factor：指定了副本个数。
--create:创建主题的动作指定
// 执行成功会显示
Created topic lixiaochi
```

### 2.展示所有主题

```
kafka-topics.sh --zookeeper localhost:2181 --list    // 展示当前创建的所有主题。
```

### 3.查看主题详情

```
kafka-topics.sh --zookeeper localhost:2181 --describe --topic lixiaochi
--describe 查看详情动作指令
```

### 4.启动消费端接收消息

```
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic lixiaochi
--bootstrap-server 指定了连接kafka集群的地址
--topic 指定了消费端订阅的主题
```

### 5.生产者发送消息

```
kafka-console-producer.sh --broker-list localhost:9092 --topic lixiaochi
--broker-list：指定了连接的kafka集群的地址
--topic 指定了发送消息时的主题
```

## 3.Java第一个程序

通过java程序来进行kafka收发信息的教学演示

### 3.1kafka配置maven依赖

​	kafka自身提供的java客户端来演示消息的收发，与kafka的Java客户端相关的Maven依赖如下。

```
<properties>
    <scala.version>2.11</scala.version>
    <slf4j.version>1.7.21</slf4j.version>
    <lombok.version>1.18.8</lombok.version>
    <junit.version>4.11</junit.version>
    <gson.version>2.2.4</gson.version>
    <protobuff.version>1.5.4</protobuff.version>
    <spark.version>2.3.1</spark.version>
	<kafka.version>2.0.0</kafka.version>
</properties>
<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
    <version>2.0.0</version>
    <!-- <version>${kafka.version}</version>-->
</dependency>

```

### 3.2创建生产者

**注意要配置主机的host文件。**

```
package com.lixiaohi.kafkastudy.mykafkatest.producer;

import kafka.tools.ConsoleProducer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

/**
 * @author lixiaochi
 * @date 2019/12/4-17:15
 * @message kafka消息生产者
 */
public class ProducerFastStart {
    // kafka集群地址
    private static final String brokerList = "101.200.194.183:9092";

    // 主题名称-之前已经创建好的主题
    private static final String topic = "lixiaochi";

    public static void main(String[] args) {
        Properties properties = new Properties();
        // 设置key序列化器
        properties.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");

        // 另外一种写法
        //properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        // 设置重试次数
        properties.put(ProducerConfig.RETRIES_CONFIG,10);

        // 设置值序列化器
        properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");

        // 设置集群地址
        properties.put("bootstrapservers",brokerList);

        // kafkaProducer 线程安全
        KafkaProducer<String,String> producer = new KafkaProducer<String, String>(properties);

        // 向kafka服务器，发送的消息
        ProducerRecord<String,String> record = new ProducerRecord<>(topic,"kafka-demo-001");

        // 生产者发送消息
        try {
            producer.send(record);
        } catch(Exception e){
            e.printStackTrace();
        } finally {
            producer.close();
        }

    }
}

```

### 3.3创建消费者

```
package com.lixiaohi.kafkastudy.mykafkatest.consumer;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.Properties;

/**
 * @author lixiaochi
 * @date 2019/12/4-17:41
 * @message kafak 消息消费者
 */
public class ConsumerFastStart {
    // kafka集群地址
    private static final String brokerList = "101.200.194.183:9092";

    // 主题名称-之前已经创建好的主题
    private static final String topic = "lixiaochi";

    // 消费组
    private static final String groupId = "group.demo";

    public static void main(String[] args) {
        Properties properties = new Properties();
        // 设置key序列化器
        properties.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");

        // 另外一种写法
        //properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        // 设置值序列化器
        properties.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");

        // 设置集群地址
        properties.put("bootstrapservers",brokerList);

        properties.put("group.id",groupId);

        KafkaConsumer<String,String> consumer = new KafkaConsumer<String, String>(properties);
        consumer.subscribe(Collections.singletonList(topic));

        while(true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            for (ConsumerRecord<String, String> record : records) {
                System.out.println(record.value());
            }
        }
    }
}

```

总结：通过本内容的学习，相信对kafka已经有了初步的了解，并且能够快速安装zookeeper、kafka组件，并且通过kafka自身命令进行消息的发送和订阅，对服务端重要参数有了初步的认识，能够通过Java客户端编写Java程序，完成消息的发布和订阅。